# [–î–æ–º–∞—à–Ω—î –∑–∞–≤–¥–∞–Ω–Ω—è –¥–æ —Ç–µ–º–∏ ¬´Apache Spark. –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è —Ç–∞ SparkU–Ü¬ª](https://www.edu.goit.global/learn/25315460/26851475/26851542/homework)

## –û–ø–∏—Å –¥–æ–º–∞—à–Ω—å–æ–≥–æ –∑–∞–≤–¥–∞–Ω–Ω—è

### –ß–∞—Å—Ç–∏–Ω–∞ 1

–ó–∞ –æ—Å–Ω–æ–≤—É –≤—ñ–∑—å–º–µ–º–æ –≤–∂–µ –∑–Ω–∞–π–æ–º–∏–π –≤–∞–º –∫–æ–¥ —ñ –¥–æ–¥–∞–º–æ –ø—Ä–æ–º—ñ–∂–Ω—É –¥—ñ—é:
```python
from pyspark.sql import SparkSession

# –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Å—ñ—é Spark
spark = SparkSession.builder \\
    .master("local[*]") \\
    .config("spark.sql.shuffle.partitions", "2") \\
    .appName("MyGoitSparkSandbox") \\
    .getOrCreate()

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç
nuek_df = spark.read \\
    .option("header", "true") \\
    .option("inferSchema", "true") \\
    .csv('./nuek-vuh3.csv')

nuek_repart = nuek_df.repartition(2)

nuek_processed = nuek_repart \\
    .where("final_priority < 3") \\
    .select("unit_id", "final_priority") \\
    .groupBy("unit_id") \\
    .count()

# –û—Å—å –¢–£–¢ –¥–æ–¥–∞–Ω–æ —Ä—è–¥–æ–∫
nuek_processed = nuek_processed.where("count>2")

nuek_processed.collect()

input("Press Enter to continue...5")

# –ó–∞–∫—Ä–∏–≤–∞—î–º–æ —Å–µ—Å—ñ—é Spark
spark.stop()
```
–ó–∞–ø—É—Å—Ç—ñ—Ç—å –∫–æ–¥. –ó—Ä–æ–±—ñ—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç —É—Å—ñ—Ö Jobs (—ó—Ö –º–∞—î –±—É—Ç–∏ 5).
### –†–µ–∑—É–ª—å—Ç–∞—Ç
![](./assets/part_1.png)

### –ß–∞—Å—Ç–∏–Ω–∞ 2

–î–æ–¥–∞–º–æ –ø—Ä–æ–º—ñ–∂–Ω–∏–π Action ‚Äî `collect`:
```python
from pyspark.sql import SparkSession

# –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Å—ñ—é Spark
spark = SparkSession.builder \\
    .master("local[*]") \\
    .config("spark.sql.shuffle.partitions", "2") \\
    .appName("MyGoitSparkSandbox") \\
    .getOrCreate()

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç
nuek_df = spark.read \\
    .option("header", "true") \\
    .option("inferSchema", "true") \\
    .csv('./nuek-vuh3.csv')

nuek_repart = nuek_df.repartition(2)

nuek_processed = nuek_repart \\
    .where("final_priority < 3") \\
    .select("unit_id", "final_priority") \\
    .groupBy("unit_id") \\
    .count()
    
# –ü—Ä–æ–º—ñ–∂–Ω–∏–π action: collect
nuek_processed.collect()

# –û—Å—å –¢–£–¢ –¥–æ–¥–∞–Ω–æ —Ä—è–¥–æ–∫
nuek_processed = nuek_processed.where("count>2")

nuek_processed.collect()

input("Press Enter to continue...5")

# –ó–∞–∫—Ä–∏–≤–∞—î–º–æ —Å–µ—Å—ñ—é Spark
spark.stop()
```
–ó–∞–ø—É—Å—Ç—ñ—Ç—å –∫–æ–¥. –ó—Ä–æ–±—ñ—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç —É—Å—ñ—Ö Jobs (—ó—Ö –º–∞—î –±—É—Ç–∏ 8).
### –†–µ–∑—É–ª—å—Ç–∞—Ç
![](./assets/part_2.png)

 üß† –ü–æ–¥—É–º–∞–π—Ç–µ, —á–æ–º—É –ø—Ä–∏ –¥–æ–¥–∞–≤–∞–Ω–Ω—ñ –æ–¥–Ω—ñ—î—ó –ø—Ä–æ–º—ñ–∂–Ω–æ—ó –¥—ñ—ó `nuek_processed.collect()`, –æ—Ç—Ä–∏–º–∞–Ω–æ –∞–∂ –Ω–∞ 3 Job –±—ñ–ª—å—à–µ?

_–í—ñ–¥–ø–æ–≤—ñ–¥—å:_ –≤–∏–∫–ª–∏–∫ `collect` –∑–º—É—Å–∏–≤ Spark –¥–≤—ñ—á—ñ –ø–æ–≤–Ω—ñ—Å—Ç—é –ø–µ—Ä–µ—Ä–∞—Ö—É–≤–∞—Ç–∏ –≤–µ—Å—å DAG (–¥–æ —Ñ—ñ–ª—å—Ç—Ä–∞, –ø—ñ—Å–ª—è), –∞ –∫–æ–∂–Ω–µ –ø–æ–≤–Ω–µ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –∑ –∫—ñ–ª—å–∫–æ—Ö Jobs - —Ü–µ –º–æ–∂–µ –±—É—Ç–∏ –ø—Ä–∏—á–∏–Ω–æ—é –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö +3 Jobs.

### –ß–∞—Å—Ç–∏–Ω–∞ 3

–í–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ –Ω–æ–≤—É –¥–ª—è –≤–∞—Å —Ñ—É–Ω–∫—Ü—ñ—é `cache` –≤ –ø—Ä–æ–º—ñ–∂–Ω–æ–º—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ.

 ‚òùüèª–§—É–Ω–∫—Ü—ñ—è `cache()` –≤ `PySpark` –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è (–∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –≤ –ø–∞–º'—è—Ç—ñ) –¥–∞–Ω–∏—Ö RDD (Resilient Distributed Dataset) –∞–±–æ `DataFrame`. –¶–µ –¥–æ–∑–≤–æ–ª—è—î –ø—Ä–∏—Å–∫–æ—Ä–∏—Ç–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –ø–æ–¥–∞–ª—å—à–∏—Ö –¥—ñ–π (actions) –∞–±–æ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω—å (transformations), —è–∫—ñ –ø—Ä–∞—Ü—é—é—Ç—å –∑ —Ç–∏–º–∏ –∂ –¥–∞–Ω–∏–º–∏. –ö–µ—à—É–≤–∞–Ω–Ω—è –æ—Å–æ–±–ª–∏–≤–æ –∫–æ—Ä–∏—Å–Ω–µ, –∫–æ–ª–∏ –≤–∏ –≤–∏–∫–æ–Ω—É—î—Ç–µ –¥–µ–∫—ñ–ª—å–∫–∞ –æ–ø–µ—Ä–∞—Ü—ñ–π –Ω–∞ –æ–¥–Ω–æ–º—É –π —Ç–æ–º—É –∂ RDD –∞–±–æ `DataFrame`, –æ—Å–∫—ñ–ª—å–∫–∏ PySpark –Ω–µ –±—É–¥–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ –æ–±—á–∏—Å–ª—é–≤–∞—Ç–∏ —Ç—ñ —Å–∞–º—ñ –¥–∞–Ω—ñ.


–Ø–∫ –ø—Ä–∞—Ü—é—î cache():

1. –ö–µ—à—É–≤–∞–Ω–Ω—è –≤ –ø–∞–º'—è—Ç—ñ. –ö–æ–ª–∏ –≤–∏ –≤–∏–∫–ª–∏–∫–∞—î—Ç–µ `cache()` –Ω–∞ RDD –∞–±–æ `DataFrame`, –¥–∞–Ω—ñ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è –≤ –ø–∞–º'—è—Ç—ñ (RAM) —É —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–æ–º—É –≤–∏–≥–ª—è–¥—ñ –Ω–∞ –≤—Å—ñ—Ö –≤—É–∑–ª–∞—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞. –¶–µ –¥–æ–∑–≤–æ–ª—è—î –ø—Ä–∏—Å–∫–æ—Ä–∏—Ç–∏ –ø–æ–¥–∞–ª—å—à—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è, –æ—Å–∫—ñ–ª—å–∫–∏ Spark –Ω–µ –±—É–¥–µ –∑–Ω–æ–≤—É –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—Ç–∏ –∞–±–æ –æ–±—á–∏—Å–ª—é–≤–∞—Ç–∏ —Ü—ñ –¥–∞–Ω—ñ.
2. –õ—ñ–Ω–∏–≤–µ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è. –í–∏–∫–ª–∏–∫ `cache()` –Ω–µ –ø—Ä–∏–∑–≤–æ–¥–∏—Ç—å –¥–æ –Ω–µ–≥–∞–π–Ω–æ–≥–æ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –æ–±—á–∏—Å–ª–µ–Ω—å. –õ–∏—à–µ –∫–æ–ª–∏ –≤–∏ –≤–∏–∫–æ–Ω—É—î—Ç–µ –¥—ñ—é (action), –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, `count()`, `collect()`, –∞–±–æ `show()`, –¥–∞–Ω—ñ –±—É–¥—É—Ç—å –æ–±—á–∏—Å–ª–µ–Ω—ñ —Ç–∞ –∫–µ—à–æ–≤–∞–Ω—ñ.
3. –ú–µ—Ö–∞–Ω—ñ–∑–º –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è. –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º, `cache()` –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –ø–∞–º'—è—Ç—å (Memory). –û–¥–Ω–∞–∫, —è–∫—â–æ –¥–∞–Ω—ñ –Ω–µ –ø–æ–º—ñ—â–∞—é—Ç—å—Å—è –≤ –ø–∞–º'—è—Ç—å, Spark –±—É–¥–µ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ —ó—Ö –Ω–∞ –¥–∏—Å–∫—É.
4. –ö–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –∫–µ—à—É–≤–∞–Ω–Ω—è–º. –ö–æ–ª–∏ –≤–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç–µ `cache()`, Spark –∑–±–µ—Ä—ñ–≥–∞—î –¥–∞–Ω—ñ –∑ —Ä—ñ–≤–Ω–µ–º –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è `MEMORY_ONLY`. –Ø–∫—â–æ –≤–∏ —Ö–æ—á–µ—Ç–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —ñ–Ω—à—ñ —Ä—ñ–≤–Ω—ñ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è, —Ç–∞–∫—ñ —è–∫ `MEMORY_AND_DISK`, –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –º–µ—Ç–æ–¥ `persist()`.


```python
from pyspark.sql import SparkSession

# –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Å—ñ—é Spark
spark = SparkSession.builder \\
    .master("local[*]") \\
    .config("spark.sql.shuffle.partitions", "2") \\
    .appName("MyGoitSparkSandbox") \\
    .getOrCreate()

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç
nuek_df = spark.read \\
    .option("header", "true") \\
    .option("inferSchema", "true") \\
    .csv('./nuek-vuh3.csv')

nuek_repart = nuek_df.repartition(2)

nuek_processed_cached = nuek_repart \\
    .where("final_priority < 3") \\
    .select("unit_id", "final_priority") \\
    .groupBy("unit_id") \\
    .count() \\
    .cache()  # –î–æ–¥–∞–Ω–æ —Ñ—É–Ω–∫—Ü—ñ—é cache

# –ü—Ä–æ–º—ñ–∂–Ω–∏–π action: collect
nuek_processed_cached.collect()

# –û—Å—å –¢–£–¢ –¥–æ–¥–∞–Ω–æ —Ä—è–¥–æ–∫
nuek_processed = nuek_processed_cached.where("count>2")

nuek_processed.collect()

input("Press Enter to continue...5")

# –ó–≤—ñ–ª—å–Ω—è—î–º–æ –ø—è–º'—è—Ç—å –≤—ñ–¥ Dataframe
nuek_processed_cached.unpersist()

# –ó–∞–∫—Ä–∏–≤–∞—î–º–æ —Å–µ—Å—ñ—é Spark
spark.stop()
```
–ó–∞–ø—É—Å—Ç—ñ—Ç—å –∫–æ–¥. –ó—Ä–æ–±—ñ—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç —É—Å—ñ—Ö Jobs (—ó—Ö –º–∞—î –±—É—Ç–∏ 7).
![](./assets/part_3.png)

 üß†–ü–æ–¥—É–º–∞–π—Ç–µ, —á–æ–º—É –ø—Ä–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—ñ `cache()` –º–∏ –∑–º–µ–Ω—à–∏–ª–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å Job?

_–í—ñ–¥–ø–æ–≤—ñ–¥—å:_ –∑–∞–ø—É—Å–∫ `cache()` –¥—Ä—É–≥–∏–π `collect` –Ω–µ –∑–º—É—à—É—î Spark –ø–æ–≤—Ç–æ—Ä–Ω–æ —á–∏—Ç–∞—Ç–∏ CSV, –≤–∏–∫–æ–Ω—É–≤–∞—Ç–∏ inferSchema —Ç–∞ —Ä–æ–±–∏—Ç–∏ shuffle-–∞–≥—Ä–µ–≥–∞—Ü—ñ—é ‚Äî –≤—ñ–Ω –º–∞—î –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –∑ —É–∂–µ –∫–µ—à–æ–≤–∞–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏. –¶–µ –º–æ–∂–µ –±—É—Ç–∏ –ø—Ä–∏—á–∏–Ω–æ—é –∑–º–µ–Ω—à–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ Jobs –¥–æ 7.

### –ö—Ä–∏—Ç–µ—Ä—ñ—ó –ø—Ä–∏–π–Ω—è—Ç—Ç—è
–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —Ç—Ä–∏ —Å–∫—Ä–∏–Ω—à–æ—Ç–∏ –∑ Jobs –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ SparkUI (http://localhost:4040/jobs/).

### –í–∏—Ö—ñ–¥–Ω–∏–π –∫–æ–¥
[results.ipynb](./scripts/results.ipynb)

### –†–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ–π
[goit-de-hw-04](https://github.com/nickolas-z/goit-de-hw-04)
