# [–î–æ–º–∞—à–Ω—î –∑–∞–≤–¥–∞–Ω–Ω—è –¥–æ —Ç–µ–º–∏ ¬´Apache Spark. –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è —Ç–∞ SparkU–Ü¬ª](https://www.edu.goit.global/learn/25315460/26851475/26851542/homework)

## –û–ø–∏—Å –¥–æ–º–∞—à–Ω—å–æ–≥–æ –∑–∞–≤–¥–∞–Ω–Ω—è

### –ß–∞—Å—Ç–∏–Ω–∞ 1

–ó–∞ –æ—Å–Ω–æ–≤—É –≤—ñ–∑—å–º–µ–º–æ –≤–∂–µ –∑–Ω–∞–π–æ–º–∏–π –≤–∞–º –∫–æ–¥ —ñ –¥–æ–¥–∞–º–æ –ø—Ä–æ–º—ñ–∂–Ω—É –¥—ñ—é:
```python
from pyspark.sql import SparkSession

# –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Å—ñ—é Spark
spark = SparkSession.builder \\
    .master("local[*]") \\
    .config("spark.sql.shuffle.partitions", "2") \\
    .appName("MyGoitSparkSandbox") \\
    .getOrCreate()

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç
nuek_df = spark.read \\
    .option("header", "true") \\
    .option("inferSchema", "true") \\
    .csv('./nuek-vuh3.csv')

nuek_repart = nuek_df.repartition(2)

nuek_processed = nuek_repart \\
    .where("final_priority < 3") \\
    .select("unit_id", "final_priority") \\
    .groupBy("unit_id") \\
    .count()

# –û—Å—å –¢–£–¢ –¥–æ–¥–∞–Ω–æ —Ä—è–¥–æ–∫
nuek_processed = nuek_processed.where("count>2")

nuek_processed.collect()

input("Press Enter to continue...5")

# –ó–∞–∫—Ä–∏–≤–∞—î–º–æ —Å–µ—Å—ñ—é Spark
spark.stop()
```

–ó–∞–ø—É—Å—Ç—ñ—Ç—å –∫–æ–¥. –ó—Ä–æ–±—ñ—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç —É—Å—ñ—Ö Jobs (—ó—Ö –º–∞—î –±—É—Ç–∏ 5).


### –ß–∞—Å—Ç–∏–Ω–∞ 2

–î–æ–¥–∞–º–æ –ø—Ä–æ–º—ñ–∂–Ω–∏–π Action ‚Äî `collect`:
```python
from pyspark.sql import SparkSession

# –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Å—ñ—é Spark
spark = SparkSession.builder \\
    .master("local[*]") \\
    .config("spark.sql.shuffle.partitions", "2") \\
    .appName("MyGoitSparkSandbox") \\
    .getOrCreate()

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç
nuek_df = spark.read \\
    .option("header", "true") \\
    .option("inferSchema", "true") \\
    .csv('./nuek-vuh3.csv')

nuek_repart = nuek_df.repartition(2)

nuek_processed = nuek_repart \\
    .where("final_priority < 3") \\
    .select("unit_id", "final_priority") \\
    .groupBy("unit_id") \\
    .count()
    
# –ü—Ä–æ–º—ñ–∂–Ω–∏–π action: collect
nuek_processed.collect()

# –û—Å—å –¢–£–¢ –¥–æ–¥–∞–Ω–æ —Ä—è–¥–æ–∫
nuek_processed = nuek_processed.where("count>2")

nuek_processed.collect()

input("Press Enter to continue...5")

# –ó–∞–∫—Ä–∏–≤–∞—î–º–æ —Å–µ—Å—ñ—é Spark
spark.stop()
```


–ó–∞–ø—É—Å—Ç—ñ—Ç—å –∫–æ–¥. –ó—Ä–æ–±—ñ—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç —É—Å—ñ—Ö Jobs (—ó—Ö –º–∞—î –±—É—Ç–∏ 8).

 üß† –ü–æ–¥—É–º–∞–π—Ç–µ, —á–æ–º—É –ø—Ä–∏ –¥–æ–¥–∞–≤–∞–Ω–Ω—ñ –æ–¥–Ω—ñ—î—ó –ø—Ä–æ–º—ñ–∂–Ω–æ—ó –¥—ñ—ó nuek_processed.collect(), –æ—Ç—Ä–∏–º–∞–Ω–æ –∞–∂ –Ω–∞ 3 Job –±—ñ–ª—å—à–µ?


### –ß–∞—Å—Ç–∏–Ω–∞ 3

–í–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ –Ω–æ–≤—É –¥–ª—è –≤–∞—Å —Ñ—É–Ω–∫—Ü—ñ—é cache –≤ –ø—Ä–æ–º—ñ–∂–Ω–æ–º—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ.

 ‚òùüèª–§—É–Ω–∫—Ü—ñ—è cache() –≤ PySpark –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è (–∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –≤ –ø–∞–º'—è—Ç—ñ) –¥–∞–Ω–∏—Ö RDD (Resilient Distributed Dataset) –∞–±–æ DataFrame. –¶–µ –¥–æ–∑–≤–æ–ª—è—î –ø—Ä–∏—Å–∫–æ—Ä–∏—Ç–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –ø–æ–¥–∞–ª—å—à–∏—Ö –¥—ñ–π (actions) –∞–±–æ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω—å (transformations), —è–∫—ñ –ø—Ä–∞—Ü—é—é—Ç—å –∑ —Ç–∏–º–∏ –∂ –¥–∞–Ω–∏–º–∏. –ö–µ—à—É–≤–∞–Ω–Ω—è –æ—Å–æ–±–ª–∏–≤–æ –∫–æ—Ä–∏—Å–Ω–µ, –∫–æ–ª–∏ –≤–∏ –≤–∏–∫–æ–Ω—É—î—Ç–µ –¥–µ–∫—ñ–ª—å–∫–∞ –æ–ø–µ—Ä–∞—Ü—ñ–π –Ω–∞ –æ–¥–Ω–æ–º—É –π —Ç–æ–º—É –∂ RDD –∞–±–æ DataFrame, –æ—Å–∫—ñ–ª—å–∫–∏ PySpark –Ω–µ –±—É–¥–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ –æ–±—á–∏—Å–ª—é–≤–∞—Ç–∏ —Ç—ñ —Å–∞–º—ñ –¥–∞–Ω—ñ.


–Ø–∫ –ø—Ä–∞—Ü—é—î cache() :

1. –ö–µ—à—É–≤–∞–Ω–Ω—è –≤ –ø–∞–º'—è—Ç—ñ. –ö–æ–ª–∏ –≤–∏ –≤–∏–∫–ª–∏–∫–∞—î—Ç–µ cache() –Ω–∞ RDD –∞–±–æ DataFrame, –¥–∞–Ω—ñ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è –≤ –ø–∞–º'—è—Ç—ñ (RAM) —É —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–æ–º—É –≤–∏–≥–ª—è–¥—ñ –Ω–∞ –≤—Å—ñ—Ö –≤—É–∑–ª–∞—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞. –¶–µ –¥–æ–∑–≤–æ–ª—è—î –ø—Ä–∏—Å–∫–æ—Ä–∏—Ç–∏ –ø–æ–¥–∞–ª—å—à—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è, –æ—Å–∫—ñ–ª—å–∫–∏ Spark –Ω–µ –±—É–¥–µ –∑–Ω–æ–≤—É –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—Ç–∏ –∞–±–æ –æ–±—á–∏—Å–ª—é–≤–∞—Ç–∏ —Ü—ñ –¥–∞–Ω—ñ.

2. –õ—ñ–Ω–∏–≤–µ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è. –í–∏–∫–ª–∏–∫ cache() –Ω–µ –ø—Ä–∏–∑–≤–æ–¥–∏—Ç—å –¥–æ –Ω–µ–≥–∞–π–Ω–æ–≥–æ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –æ–±—á–∏—Å–ª–µ–Ω—å. –õ–∏—à–µ –∫–æ–ª–∏ –≤–∏ –≤–∏–∫–æ–Ω—É—î—Ç–µ –¥—ñ—é (action), –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, count(), collect(), –∞–±–æ show(), –¥–∞–Ω—ñ –±—É–¥—É—Ç—å –æ–±—á–∏—Å–ª–µ–Ω—ñ —Ç–∞ –∫–µ—à–æ–≤–∞–Ω—ñ.

3. –ú–µ—Ö–∞–Ω—ñ–∑–º –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è. –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º, cache() –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –ø–∞–º'—è—Ç—å (Memory). –û–¥–Ω–∞–∫, —è–∫—â–æ –¥–∞–Ω—ñ –Ω–µ –ø–æ–º—ñ—â–∞—é—Ç—å—Å—è –≤ –ø–∞–º'—è—Ç—å, Spark –±—É–¥–µ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ —ó—Ö –Ω–∞ –¥–∏—Å–∫—É.

4. –ö–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –∫–µ—à—É–≤–∞–Ω–Ω—è–º. –ö–æ–ª–∏ –≤–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç–µ cache(), Spark –∑–±–µ—Ä—ñ–≥–∞—î –¥–∞–Ω—ñ –∑ —Ä—ñ–≤–Ω–µ–º –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è MEMORY_ONLY. –Ø–∫—â–æ –≤–∏ —Ö–æ—á–µ—Ç–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —ñ–Ω—à—ñ —Ä—ñ–≤–Ω—ñ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è, —Ç–∞–∫—ñ —è–∫ MEMORY_AND_DISK, –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –º–µ—Ç–æ–¥ persist().

‚òùüèª–ê–ª–µ –≤ —Ç–∞–∫—ñ –¥–µ—Ç–∞–ª—ñ –º–∏ –∑–∞–Ω—É—Ä—é–≤–∞—Ç–∏—Å—è –Ω–∞ –ø—Ä–∞–∫—Ç–∏—Ü—ñ –Ω–µ –±—É–¥–µ–º–æ. –í–∞–∂–ª–∏–≤–æ –∑–Ω–∞—Ç–∏, —â–æ –º–æ–∂–ª–∏–≤–æ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ —è–∫ –≤ –ø–∞–º‚Äô—è—Ç—ñ, —Ç–∞–∫ —ñ –Ω–∞ –¥–∏—Å–∫—É. –ó–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –≤ –ø–∞–º‚Äô—è—Ç—ñ –Ω–∞–±–∞–≥–∞—Ç–æ –±—ñ–ª—å—à —Ä–æ–∑–ø–æ–≤—Å—é–¥–∂–µ–Ω–µ, –Ω–∞ –¥–∏—Å–∫—É ‚Äî –µ–∫–∑–æ—Ç–∏–∫–∞ üòâ
```python
from pyspark.sql import SparkSession

# –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Å—ñ—é Spark
spark = SparkSession.builder \\
    .master("local[*]") \\
    .config("spark.sql.shuffle.partitions", "2") \\
    .appName("MyGoitSparkSandbox") \\
    .getOrCreate()

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç
nuek_df = spark.read \\
    .option("header", "true") \\
    .option("inferSchema", "true") \\
    .csv('./nuek-vuh3.csv')

nuek_repart = nuek_df.repartition(2)

nuek_processed_cached = nuek_repart \\
    .where("final_priority < 3") \\
    .select("unit_id", "final_priority") \\
    .groupBy("unit_id") \\
    .count() \\
    .cache()  # –î–æ–¥–∞–Ω–æ —Ñ—É–Ω–∫—Ü—ñ—é cache

# –ü—Ä–æ–º—ñ–∂–Ω–∏–π action: collect
nuek_processed_cached.collect()

# –û—Å—å –¢–£–¢ –¥–æ–¥–∞–Ω–æ —Ä—è–¥–æ–∫
nuek_processed = nuek_processed_cached.where("count>2")

nuek_processed.collect()

input("Press Enter to continue...5")

# –ó–≤—ñ–ª—å–Ω—è—î–º–æ –ø—è–º'—è—Ç—å –≤—ñ–¥ Dataframe
nuek_processed_cached.unpersist()

# –ó–∞–∫—Ä–∏–≤–∞—î–º–æ —Å–µ—Å—ñ—é Spark
spark.stop()
```


–ó–∞–ø—É—Å—Ç—ñ—Ç—å –∫–æ–¥. –ó—Ä–æ–±—ñ—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç —É—Å—ñ—Ö Jobs (—ó—Ö –º–∞—î –±—É—Ç–∏ 7).

 üß†–ü–æ–¥—É–º–∞–π—Ç–µ, —á–æ–º—É –ø—Ä–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—ñ cache() –º–∏ –∑–º–µ–Ω—à–∏–ª–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å Job?

### –ö—Ä–∏—Ç–µ—Ä—ñ—ó –ø—Ä–∏–π–Ω—è—Ç—Ç—è

–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —Ç—Ä–∏ —Å–∫—Ä–∏–Ω—à–æ—Ç–∏ –∑ Jobs –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ SparkUI (http://localhost:4040/jobs/).

## –†—ñ—à–µ–Ω–Ω—è


### –†–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ–π
[goit-de-hw-04](https://github.com/nickolas-z/goit-de-hw-04)
